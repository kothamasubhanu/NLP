{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bc4c5a-e9c2-4a88-842e-3a23513af556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bigram Frequency: 1.03\n",
      "\n",
      "|   Rank | Bigram                 |   Frequency |   Mean Probability (μ-value) |\n",
      "|--------+------------------------+-------------+------------------------------|\n",
      "|      1 | ('clever', 'fox')      |           2 |                     0.011429 |\n",
      "|      2 | ('forest', 'full')     |           2 |                     0.011429 |\n",
      "|      3 | ('forest', 'fox')      |           2 |                     0.011429 |\n",
      "|      4 | ('realized', 'forest') |           2 |                     0.011429 |\n",
      "|      5 | ('new', 'friends')     |           2 |                     0.011429 |\n",
      "|      6 | ('upon', 'time')       |           1 |                     0.005714 |\n",
      "|      7 | ('time', 'lush')       |           1 |                     0.005714 |\n",
      "|      8 | ('lush', 'green')      |           1 |                     0.005714 |\n",
      "|      9 | ('green', 'forest')    |           1 |                     0.005714 |\n",
      "|     10 | ('forest', 'lived')    |           1 |                     0.005714 |\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from tabulate import tabulate\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesses the text by tokenizing, removing punctuation and stopwords.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_mean_probability(bigram_freq: FreqDist, total_bigrams: int) -> dict:\n",
    "    \"\"\"Calculates the mean probability (μ-value) of each bigram.\"\"\"\n",
    "    mean_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return mean_probabilities\n",
    "\n",
    "def main():\n",
    "    # Download required NLTK data\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "\n",
    "    # Load text\n",
    "    with open(r\"E:\\126156072\\NLP\\sample.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess text\n",
    "    words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    # Calculate bigrams and their frequencies\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    # Calculate mean probability (μ-value) for each bigram\n",
    "    mean_probabilities = calculate_mean_probability(bigram_freq, len(bigrams))\n",
    "\n",
    "    # Sort collocations by mean probability\n",
    "    collocations = sorted(mean_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate mean of bigram frequencies\n",
    "    total_bigram_freq = sum(bigram_freq.values())\n",
    "    mean_bigram_freq = total_bigram_freq / len(bigram_freq)\n",
    "\n",
    "    # Print mean bigram frequency\n",
    "    print(f\"Mean Bigram Frequency: {mean_bigram_freq:.2f}\\n\")\n",
    "\n",
    "    # Print top N collocations with their frequencies and mean probabilities\n",
    "    N = 10\n",
    "    headers = [\"Rank\", \"Bigram\", \"Frequency\", \"Mean Probability (μ-value)\"]\n",
    "    table = []\n",
    "    for i, (bigram, mean_prob) in enumerate(collocations[:N]):\n",
    "        table.append([i+1, bigram, bigram_freq[bigram], f\"{mean_prob:.6f}\"])\n",
    "    print(tabulate(table, headers, tablefmt=\"orgtbl\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547267d0-4ff0-4c38-8a24-3cf6b9743c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bigram Frequency: 1.17\n",
      "\n",
      "|   Rank | Bigram                         |   Frequency |   Mean Probability (μ-value) |   t-Statistic |   p-Value (t-Test) |   Chi2 Statistic |   p-Value (Chi-Square) |\n",
      "|--------+--------------------------------+-------------+------------------------------+---------------+--------------------+------------------+------------------------|\n",
      "|      1 | ('impact', 'artificial')       |           2 |                     0.002699 |           inf |                  0 |         183.747  |                 0      |\n",
      "|      2 | ('artificial', 'intelligence') |           3 |                     0.004049 |           inf |                  0 |         307.077  |                 0      |\n",
      "|      3 | ('intelligence', 'data')       |           3 |                     0.004049 |           inf |                  0 |          12.4097 |                 0.0004 |\n",
      "|      4 | ('data', 'science')            |          15 |                     0.020243 |           inf |                  0 |         167.483  |                 0      |\n",
      "|      5 | ('science', 'introduction')    |           1 |                     0.00135  |           inf |                  0 |          11.6208 |                 0.0007 |\n",
      "|      6 | ('introduction', 'recent')     |           1 |                     0.00135  |           inf |                  0 |         184.75   |                 0      |\n",
      "|      7 | ('recent', 'years')            |           1 |                     0.00135  |           inf |                  0 |         184.75   |                 0      |\n",
      "|      8 | ('years', 'convergence')       |           1 |                     0.00135  |           inf |                  0 |         184.75   |                 0      |\n",
      "|      9 | ('convergence', 'artificial')  |           1 |                     0.00135  |           inf |                  0 |          61.0835 |                 0      |\n",
      "|     10 | ('intelligence', 'ai')         |           1 |                     0.00135  |           inf |                  0 |           0.4959 |                 0.4813 |\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_1samp, chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesses the text by tokenizing, removing punctuation and stopwords.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_mean_probability(bigram_freq: FreqDist, total_bigrams: int) -> dict:\n",
    "    \"\"\"Calculates the mean probability (μ-value) of each bigram.\"\"\"\n",
    "    mean_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return mean_probabilities\n",
    "\n",
    "def perform_statistical_tests(bigram_freq: FreqDist, word_freq: FreqDist, total_bigrams: int):\n",
    "    \"\"\"Perform t-test and chi-square test for each bigram.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for bigram, observed_freq in bigram_freq.items():\n",
    "        word1, word2 = bigram\n",
    "        freq_w1 = word_freq.get(word1, 0)\n",
    "        freq_w2 = word_freq.get(word2, 0)\n",
    "        \n",
    "        # Expected frequency for the bigram assuming independence\n",
    "        expected_freq = (freq_w1 * freq_w2) / total_bigrams\n",
    "        \n",
    "        # Chi-square test\n",
    "        observed = np.array([\n",
    "            [observed_freq, freq_w1 - observed_freq],\n",
    "            [freq_w2 - observed_freq, total_bigrams - (freq_w1 + freq_w2 - observed_freq)]\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chi2_stat, p_value_chi2, dof, ex = chi2_contingency(observed)\n",
    "        except ValueError:\n",
    "            chi2_stat, p_value_chi2 = np.nan, np.nan\n",
    "        \n",
    "        # Generate a list of observed frequencies to simulate multiple observations for t-test\n",
    "        observed_frequencies = [observed_freq] * 10  # Simulating 10 observations\n",
    "        \n",
    "        # Perform one-sample t-test\n",
    "        t_stat, p_value_t = ttest_1samp(observed_frequencies, expected_freq)\n",
    "        \n",
    "        results.append((bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Download required NLTK data\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "\n",
    "    # Load text\n",
    "    with open(r\"E:\\126156072\\NLP\\text1.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess text\n",
    "    words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Calculate bigrams and their frequencies\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    # Calculate mean probability (μ-value) for each bigram\n",
    "    mean_probabilities = calculate_mean_probability(bigram_freq, len(bigrams))\n",
    "\n",
    "    # Sort collocations by mean probability\n",
    "    collocations = sorted(mean_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate mean of bigram frequencies\n",
    "    total_bigram_freq = sum(bigram_freq.values())\n",
    "    mean_bigram_freq = total_bigram_freq / len(bigram_freq)\n",
    "\n",
    "    # Print mean bigram frequency\n",
    "    print(f\"Mean Bigram Frequency: {mean_bigram_freq:.2f}\\n\")\n",
    "\n",
    "    # Perform statistical tests for each bigram\n",
    "    results = perform_statistical_tests(bigram_freq, word_freq, len(bigrams))\n",
    "\n",
    "    # Print top N collocations with their frequencies and mean probabilities\n",
    "    N = 10\n",
    "    headers = [\"Rank\", \"Bigram\", \"Frequency\", \"Mean Probability (μ-value)\", \"t-Statistic\", \"p-Value (t-Test)\", \"Chi2 Statistic\", \"p-Value (Chi-Square)\"]\n",
    "    table = []\n",
    "    for i, (bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2) in enumerate(results[:N]):\n",
    "        table.append([\n",
    "            i+1, \n",
    "            bigram, \n",
    "            observed_freq, \n",
    "            f\"{mean_probabilities.get(bigram, 0):.6f}\", \n",
    "            f\"{t_stat:.4f}\", \n",
    "            f\"{p_value_t:.4f}\", \n",
    "            f\"{chi2_stat:.4f}\", \n",
    "            f\"{p_value_chi2:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    print(tabulate(table, headers, tablefmt=\"orgtbl\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788bc46b-8375-4271-a857-7c8b299fe20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Bigram                      |   Frequency |   Mean Probability (μ-value) |   t-Statistic |   p-Value (t-Test) |   Chi2 Statistic |   p-Value (Chi-Square) |\n",
      "|-----------------------------+-------------+------------------------------+---------------+--------------------+------------------+------------------------|\n",
      "| ('data', 'science')         |          15 |                     0.020243 |           inf |                  0 |         167.725  |                 0      |\n",
      "| ('data', 'processing')      |           7 |                     0.009447 |           inf |                  0 |          32.6757 |                 0      |\n",
      "| ('predictive', 'analytics') |           5 |                     0.006748 |           inf |                  0 |         499.265  |                 0      |\n",
      "| ('data', 'visualization')   |           5 |                     0.006748 |           inf |                  0 |          37.8899 |                 0      |\n",
      "| ('ai', 'data')              |           4 |                     0.005398 |           inf |                  0 |           0.7573 |                 0.3842 |\n",
      "| ('ai', 'algorithms')        |           4 |                     0.005398 |           inf |                  0 |          29.6835 |                 0      |\n",
      "| ('data', 'cleaning')        |           4 |                     0.005398 |           inf |                  0 |          35.4377 |                 0      |\n",
      "| ('cleaning', 'preparation') |           4 |                     0.005398 |           inf |                  0 |         567.214  |                 0      |\n",
      "| ('natural', 'language')     |           4 |                     0.005398 |           inf |                  0 |         452.98   |                 0      |\n",
      "| ('language', 'processing')  |           4 |                     0.005398 |           inf |                  0 |         136.214  |                 0      |\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp, chi2_contingency\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Download the necessary datasets if you haven't already\n",
    "\n",
    "# Define the set of English stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# Function to process the text file\n",
    "def process_text_file(file_path):\n",
    "    \"\"\"Reads the text from the file, tokenizes, and filters stopwords.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in english_stops and word.isalpha()]\n",
    "    return filtered_words\n",
    "\n",
    "# Function to calculate word and bigram frequencies\n",
    "def calculate_frequencies(words):\n",
    "    \"\"\"Calculates word and bigram frequencies.\"\"\"\n",
    "    word_freq = Counter(words)\n",
    "    bigram_list = list(bigrams(words))\n",
    "    bigram_freq = Counter(bigram_list)\n",
    "    return word_freq, bigram_freq\n",
    "\n",
    "# Function to calculate the mean frequency of each bigram\n",
    "def calculate_bigram_means(bigram_freq, total_bigrams):\n",
    "    \"\"\"Calculates the mean frequency of each bigram.\"\"\"\n",
    "    bigram_means = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return bigram_means\n",
    "\n",
    "# Function to perform t-Test and Chi-Square test for each bigram\n",
    "def perform_statistical_tests(filtered_words):\n",
    "    \"\"\"Performs t-Test and Chi-Square test for each bigram.\"\"\"\n",
    "    bigram_list = list(bigrams(filtered_words))\n",
    "    bigram_freq = Counter(bigram_list)\n",
    "    word_freq = Counter(filtered_words)\n",
    "    corpus_size = len(filtered_words)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for bigram, freq_bi in bigram_freq.items():\n",
    "        freq_w1 = word_freq[bigram[0]]\n",
    "        freq_w2 = word_freq[bigram[1]]\n",
    "        expected_freq = (freq_w1 * freq_w2) / corpus_size\n",
    "        \n",
    "        if freq_bi == 0:\n",
    "            freq_bi = 1\n",
    "        \n",
    "        observed = np.array([\n",
    "            [freq_bi, freq_w1 - freq_bi],\n",
    "            [freq_w2 - freq_bi, corpus_size - (freq_w1 + freq_w2 - freq_bi)]\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chi2_stat, p_value_chi2, dof, ex = chi2_contingency(observed)\n",
    "        except ValueError:\n",
    "            chi2_stat, p_value_chi2 = np.nan, np.nan\n",
    "        \n",
    "        observed_frequencies = [freq_bi] * 10\n",
    "        t_stat, p_value_t = ttest_1samp(observed_frequencies, expected_freq)\n",
    "        \n",
    "        results.append((bigram, freq_bi, t_stat, p_value_t, chi2_stat, p_value_chi2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Path to the text file\n",
    "    file_path = r\"E:\\126156072\\NLP\\text1.txt\"\n",
    "    \n",
    "    # Process the text file\n",
    "    filtered_words = process_text_file(file_path)\n",
    "    \n",
    "    # Calculate frequencies\n",
    "    word_freq, bigram_freq = calculate_frequencies(filtered_words)\n",
    "    \n",
    "    # Calculate total number of bigrams\n",
    "    total_bigrams = sum(bigram_freq.values())\n",
    "    \n",
    "    # Calculate mean frequency of each bigram\n",
    "    bigram_means = calculate_bigram_means(bigram_freq, total_bigrams)\n",
    "    \n",
    "    # Sort bigrams by mean probability and get the top 5\n",
    "    top_bigrams = sorted(bigram_means.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Perform statistical tests\n",
    "    results = perform_statistical_tests(filtered_words)\n",
    "    \n",
    "    # Filter results to include only the top 5 bigrams\n",
    "    results_dict = {bigram: (freq_bi, t_stat, p_value_t, chi2_stat, p_value_chi2) for bigram, freq_bi, t_stat, p_value_t, chi2_stat, p_value_chi2 in results}\n",
    "    \n",
    "    filtered_results = [(bigram, results_dict[bigram][0], results_dict[bigram][1], results_dict[bigram][2], results_dict[bigram][3], results_dict[bigram][4]) for bigram, _ in top_bigrams]\n",
    "    \n",
    "    # Print results\n",
    "    headers = [ \"Bigram\", \"Frequency\", \"Mean Probability (μ-value)\", \"t-Statistic\", \"p-Value (t-Test)\", \"Chi2 Statistic\", \"p-Value (Chi-Square)\"]\n",
    "    table = []\n",
    "    for i, (bigram, freq_bi, t_stat, p_value_t, chi2_stat, p_value_chi2) in enumerate(filtered_results):\n",
    "        table.append([\n",
    "          \n",
    "            bigram, \n",
    "            freq_bi, \n",
    "            f\"{bigram_means.get(bigram, 0):.6f}\", \n",
    "            f\"{t_stat:.4f}\", \n",
    "            f\"{p_value_t:.4f}\", \n",
    "            f\"{chi2_stat:.4f}\", \n",
    "            f\"{p_value_chi2:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    print(tabulate(table, headers, tablefmt=\"orgtbl\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "717b4d8b-b7a6-49d3-bce4-f791d9f0915d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bigram Frequency: 1.03\n",
      "\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|   Rank | Bigram                 |   Frequency |   Mean Prob(μ) |   t-Statistic |   p-Value(t-Test) |   Chi Square |   p-Value(Chi-Square) |\n",
      "+========+========================+=============+================+===============+===================+==============+=======================+\n",
      "|      1 | ('upon', 'time')       |           1 |       0.005714 |        1      |            0.3187 |      43.2486 |                0      |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      2 | ('time', 'lush')       |           1 |       0.005714 |        1      |            0.3187 |      43.2486 |                0      |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      3 | ('lush', 'green')      |           1 |       0.005714 |        1      |            0.3187 |      43.2486 |                0      |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      4 | ('green', 'forest')    |           1 |       0.005714 |        1      |            0.3187 |       3.6611 |                0.0557 |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      5 | ('forest', 'lived')    |           1 |       0.005714 |        1      |            0.3187 |       0.6796 |                0.4097 |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      6 | ('lived', 'clever')    |           1 |       0.005714 |        1      |            0.3187 |       6.5107 |                0.0107 |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      7 | ('clever', 'fox')      |           2 |       0.011429 |        1.4183 |            0.1579 |      11.3919 |                0.0007 |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      8 | ('fox', 'fox')         |           1 |       0.005714 |       -1      |            0.3187 |       0      |                1      |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      9 | ('fox', 'known')       |           1 |       0.005714 |        1      |            0.3187 |       2.2027 |                0.1378 |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|     10 | ('known', 'quickness') |           1 |       0.005714 |        1      |            0.3187 |      43.2486 |                0      |\n",
      "+--------+------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import chi2_contingency, ttest_1samp\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesses the text by tokenizing, removing punctuation and stopwords.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_mean_probability(bigram_freq: FreqDist, total_bigrams: int) -> dict:\n",
    "    \"\"\"Calculates the mean probability (μ-value) of each bigram.\"\"\"\n",
    "    mean_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return mean_probabilities\n",
    "\n",
    "def perform_statistical_tests(bigram_freq: FreqDist, word_freq: FreqDist, total_bigrams: int):\n",
    "    \"\"\"Perform t-test and chi-square test for each bigram.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for bigram, observed_freq in bigram_freq.items():\n",
    "        word1, word2 = bigram\n",
    "        freq_w1 = word_freq.get(word1, 0)\n",
    "        freq_w2 = word_freq.get(word2, 0)\n",
    "        \n",
    "        # Expected frequency for the bigram assuming independence\n",
    "        expected_freq = (freq_w1 * freq_w2) / total_bigrams\n",
    "        \n",
    "        # Chi-square test\n",
    "        observed = np.array([\n",
    "            [observed_freq, freq_w1 - observed_freq],\n",
    "            [freq_w2 - observed_freq, total_bigrams - (freq_w1 + freq_w2 - observed_freq)]\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chi2_stat, p_value_chi2, dof, ex = chi2_contingency(observed)\n",
    "        except ValueError:\n",
    "            chi2_stat, p_value_chi2 = np.nan, np.nan\n",
    "        \n",
    "        # Generate sample data to perform t-test\n",
    "        sample_data = [observed_freq] * observed_freq + [expected_freq] * (total_bigrams - observed_freq)\n",
    "        \n",
    "        # Perform one-sample t-test\n",
    "        t_stat, p_value_t = ttest_1samp(sample_data, expected_freq)\n",
    "        \n",
    "        results.append((bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Download required NLTK data\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "\n",
    "    # Load text\n",
    "    with open(r\"E:\\126156072\\NLP\\sample.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess text\n",
    "    words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Calculate bigrams and their frequencies\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    # Calculate mean probability (μ-value) for each bigram\n",
    "    mean_probabilities = calculate_mean_probability(bigram_freq, len(bigrams))\n",
    "\n",
    "    # Sort collocations by mean probability\n",
    "    collocations = sorted(mean_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate mean of bigram frequencies\n",
    "    total_bigram_freq = sum(bigram_freq.values())\n",
    "    mean_bigram_freq = total_bigram_freq / len(bigram_freq)\n",
    "\n",
    "    # Print mean bigram frequency\n",
    "    print(f\"Mean Bigram Frequency: {mean_bigram_freq:.2f}\\n\")\n",
    "\n",
    "    # Perform statistical tests for each bigram\n",
    "    results = perform_statistical_tests(bigram_freq, word_freq, len(bigrams))\n",
    "\n",
    "    # Print top N collocations with their frequencies and mean probabilities\n",
    "    N = 10\n",
    "    headers = [\"Rank\", \"Bigram\", \"Frequency\", \"Mean Prob(μ)\", \"t-Statistic\", \"p-Value(t-Test)\", \"Chi Square\", \"p-Value(Chi-Square)\"]\n",
    "    table = []\n",
    "    for i, (bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2) in enumerate(results[:N]):\n",
    "        table.append([\n",
    "            i + 1,\n",
    "            bigram,\n",
    "            observed_freq,\n",
    "            f\"{mean_probabilities.get(bigram, 0):.6f}\",\n",
    "            f\"{t_stat:.4f}\" if not np.isnan(t_stat) else \"NaN\",\n",
    "            f\"{p_value_t:.4f}\" if not np.isnan(p_value_t) else \"NaN\",\n",
    "            f\"{chi2_stat:.4f}\" if not np.isnan(chi2_stat) else \"NaN\",\n",
    "            f\"{p_value_chi2:.4f}\" if not np.isnan(p_value_chi2) else \"NaN\"\n",
    "        ])\n",
    "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ebd8c-bd44-4ab2-b6cb-2b89f374d597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
